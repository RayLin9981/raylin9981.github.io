{"pages":[{"title":"page test","text":"","link":"/page-test/index-1.html"},{"title":"page test","text":"這是 H1 的1相關指令：chmod chown umask","link":"/page-test/index.html"}],"posts":[{"title":"2022 CKA + CKAD 考試分享","text":"前言大約花了一個月左右的時間準備，在上禮拜拿到了 CKA (Certified Kubernetes Administrator)，CKAD (Kubernetes Certified Application Developer) 兩張證照，分享一下我的準備方式與考試流程，希望可以幫助到需要的人 ^^ CKA + CKAD 簡介這兩張證照是 Linux Foundation 推出針對 Kubernetes (下稱 K8s) 管理技能的證照，關於 K8s 的系列證照還有另一張 CKS，三張難度粗略排序如下：CKS &gt; CKA &gt; CKAD，其中 CKS 必須要有 CKA 的認證才能考。以下用一句話簡述這三張證照的範圍： CKA：管理整個 K8s 叢集，包含安裝，排錯，設定，管理資源等等。適用於 K8s 管理者 CKAD：透過 K8s 來部署應用程式。適用於軟體開發者並且要使用 K8s 平台的 CKS：強調 K8s 安全上的設定，以及性能上的最佳化 目前認證考試價格為 395 USD，有提供一次免費重考的機會。網路上可以找到一些折扣碼輸入，我是使用 35% off 的，刷卡費就大約 8000 NTD 左右。考試的方式都是實作題，使用提供的環境做到題目需求的內容。因為考試有經過幾次更新，有些情況跟一些部落格的有點不太一樣了，下面會再分享我這次考試的情況。 準備方式我是直接無腦的在 udemy 上面買課程，udemy 蠻常特價的，可以等特價時入手 (我看過最低折到 NT 330 的)選擇的課是評價最高的兩堂，應該是 2019~2020 年左右推出的，不過若考試範圍有更新他也有更新資訊，就以我這次學習與考試上我認為內容是很足以應付考試的。udemy 課程連結這上面是 CKA 的，可以根據同作者去搜尋到 CKAD 的，課程長度每堂約 20 hr，是英文授課不過可以使用簡體中文字幕。針對完全沒接觸過 K8s 的學生，他有另一堂課是介紹一些最基本 K8s 的概念，但我自己是已經有使用經驗了所以就沒有再去學那堂課。另外很推薦這堂課有提供 Lab 環境可以操作，幾乎每一個觀念講完後都可以實際練習，我認為對於證照的準備是有很大的幫助。 我的準備流程買了以上兩堂課後，我是先把 CKA 的內容看完，也練完 killer.sh 提供的模擬考之後，才開始看 CKAD，但過程中發現 CKAD 的內容幾乎 CKA 都有涵蓋到，只是針對 Pod 相關多延伸了一點點，就沒有再回頭把 CKAD 整個課程跑過，而是直接去考 CKAD killer.sh 的模擬考題，發現可以大約完成 7 成左右的題目後，就只針對這個模擬考去準備了。 2022 考試流程參考我是使用 macbook pro 13 吋 2022 考的，並且有外接滑鼠 (原本還打算接一個 27 吋的螢幕)，考試流程有點沒有很順利，記錄一下我遇到的問題： 目前 2022 的版本已經不是使用 chrome 考了，而是使用 PSI Bridge Secure Browser 有許多 blog 會建議可以設書籤快速查找文件，不過現在規定要使用它的 web browser 來考試，環境會利用這個 APP 的介面操作遠端桌面，查找文件就使用裡面的 firefox 瀏覽器。 介面的使用感覺就與 killer.sh 提供的模擬介面很相似，所以只要模擬題做熟就沒什麼問題 PSI Bridge Secure Browser 與 PSI Secure Browser 這個 PSI Bridge Secure Browser 是我在考試前 30 分鐘等到 take exam 之後才發現需要安裝，大約 600 MB 左右，安裝完之後就會進入檢查設定的部分 一開始 launch exam 的時候那個 Download 不能按，所以我就自己到網路上安裝錯的 PSI Secure Browser ，搞得我很緊張 (那個頁面會要輸入 exam code，我找半天找不到) macbook 外接螢幕 一開始我打算用鏡像輸出的方式接螢幕，結果會偵測到有 2 個螢幕導致無法通過 APP 本身的安全測試，後來就先把螢幕拔掉。 等到護照，環境等等都驗證完之後，我用 Live chat 詢問能不能使用外接螢幕，他回答我 Yes，結果我一插上去 APP 就顯示我有額外接入 device，強制關掉我的考試環境 最後重新進入，又要再檢查一次護照與環境，等到都 ok 之後進去，發現我 CKA 的時間已經過了 20 分鐘，後來也沒有使用外接螢幕考試了。 後來 CKAD 的時候我就沒有打算外接螢幕，也發現那個 PSI Bridge Secure Browser 有可以調整 view 的選項，在上面的地方可以選擇 zoom in/out，把整個畫面調大一點操作起來才比較順 (相信我，不調的話那個大小根本沒辦法看文件) 考試結果通知官方是說會在 24 小時內通知，我兩個考試的結果都是在 ‘安排考試的時間點的 24 hr 後’ 收到，也就是我安排早上 9 點考試，那就是在隔天的早上 9 點收到結果，在下午 4 點就是隔天的下午 4 點這樣 後記以上就是我考到這兩張證照的心得分享，希望能幫到看完的你~~","link":"/2022/09/25/2022-CKA-CKAD-%E8%80%83%E8%A9%A6%E5%88%86%E4%BA%AB/"},{"title":"Prometheus-operator (prometheus-stack) 簡介","text":"前言目前使用 Helm 來安裝時採用的方法，透過自定義 CRD (Kubernetes Custom Resource) 來生成 prometheus 的各種元件 也因為使用 CRD 的關係，學習上會比較不直觀，這部分還是需要對 prometheus 官方文件熟悉一點預計之後會再針對基本的 prometheus, alertmanager 設定再整理一下 會把 API 存在 monitoring.coreos.com/v1 helm chart 連結 123456helm repo add prometheus-community https://prometheus-community.github.io/helm-chartshelm repo updatehelm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace# 安裝完之後進入介面 輸入以下指令之後透過 http://localhost:8080 進入kubectl port-forward -n=monitoring svc/kube-prometheus-stack-grafana 8080:80# 需要帳號密碼，透過 secret kube-prometheus-stack-grafana 取得 這個 stack 包含了很多東西：Prometheus, Prometheus Operator, Alertmanager, Grafana, kube-state-metrics, prometheus-node-exporter …等元件 以下是主要額外定義的 CRD： Prometheus：聲明式和管理 Prometheus Server 實例； config 使用 secret 存放，為 .yaml.gz 格式 (可用 gzip -d 解壓) ServiceMonitor：負責聲明式的管理監控配置； 依照設定的內容產生 jobs config 內容，並且更新到 Prometheus PrometheusRule：負責聲明式的管理告警配置； 定義 prometheus rules 使用 (新增於 configmap) Rule 的設定是透過 configmap 掛給 Prometheus 的，名稱為 prometheus-prometheus-operator-kube-p-prometheus-rulefiles-0 Alertmanager：聲明式的建立和管理 Alertmanager 實例。 使用 alertmanagerconfigs 丟入設定檔動態載入 Prometheus 這個 Pod 會跑 2 個 container，image 分別為： prometheus-config-reloader 負責監控 secret 物件，一有變動就產生新的 config 可看到 prometheus-config-reloader 的執行指令參數 --watch-dir 就是指定這個 configmap 掛載的點，因此只要新增 configmap 就可以新增 rule prometheus 實際跑 server 的 container，是讀取 config_out 目錄上的設定檔 prometheus 的設定是使用 secret 的方式來掛載，經過 initcontainer 處理後轉成可使用的 config，目錄在 /etc/prometheus/config_out/prometheus.env.yaml config：由 secret 掛載 config_out (實際載入的)：emptyDir 共用 (由 reloader 產生) rule_files：由 configMap 掛載 使用 PrometheusRule 來新增，他會去更新 configMap 在 Prometheus stack 的系統中，都是透過 labels 來連接的，以下幾個例子： Prometheus instance ↔ ServiceMonitors (產生對應 jobs) ServiceMonitors ↔ Service (設定 metrices 位置) PrometheusRule ↔ Prometheus instance (ruleSelector) ServiceMonitor透過綁定 Service 來監控 APP，綁定的方式是使用 label 加上對應的 port Name 以下範例是已經有建立了一個 deployment 與 svc 該 deployment 的 image 設計已經對 /metrics 丟了一些資料，因此沒有利用到額外的 exporter 123456789101112131415161718192021apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata: name: example-app namespace: monitoring labels: team: frontendspec: # 設定搜尋的 namespace, 可透過 any 代表所有 namespace namespaceSelector: matchNames: - default # 對應到 service 的 label selector: matchLabels: app: example-app # endpoints: 對應到 service 的 port 名稱 endpoints: - port: web path: /metrics # 為預設值 interval: 15s 設定完後，就可以在 prometheus 上看到新的 target 抓取該 APP 的 metrics Prometheus instance這部分比較不會調整到，可聚焦在 label selector 的部分 有 serviceMonitorNamespaceSelector, podMonitorSelector ruleSelector 初始化可以使用 values.yaml 調整，也可以直接 kubectl edit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata: annotations: meta.helm.sh/release-name: prometheus-operator meta.helm.sh/release-namespace: monitoring generation: 1 labels: app: kube-prometheus-stack-prometheus app.kubernetes.io/instance: prometheus-operator app.kubernetes.io/managed-by: Helm app.kubernetes.io/part-of: kube-prometheus-stack app.kubernetes.io/version: 28.0.1 chart: kube-prometheus-stack-28.0.1 heritage: Helm release: prometheus-operator name: prometheus-operator-kube-p-prometheus namespace: monitoringspec: additionalScrapeConfigs: key: additional-scrape-configs.yaml name: prometheus-operator-kube-p-prometheus-scrape-confg alerting: alertmanagers: - apiVersion: v2 name: prometheus-operator-kube-p-alertmanager namespace: monitoring pathPrefix: / port: http-web externalUrl: http://prometheus-sit.k8sbridge.com/ image: quay.io/prometheus/prometheus:v2.32.1 podMonitorNamespaceSelector: {} podMonitorSelector: {} portName: http-web probeNamespaceSelector: {} probeSelector: matchLabels: release: prometheus-operator replicas: 1 retention: 10d routePrefix: / ruleNamespaceSelector: {} ruleSelector: matchLabels: release: prometheus-operator secrets: - etcd-certs serviceAccountName: prometheus-operator-kube-p-prometheus serviceMonitorNamespaceSelector: {} serviceMonitorSelector: {} version: v2.32.1 PrometheusRule產生 jobs，內容與原本的 prometheus rule 很接近 123456789101112131415161718apiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata: labels: release: prometheus-operator name: kube-prometheus-stack-ray.rulesspec: groups: - name: ray.rules rules: - alert: masterNodeDiskChecking annotations: message: | haha! I am Ray and I learning prometheus~~ expr: node_filesystem_avail_bytes{device=&quot;/dev/mapper/ubuntu--vg-ubuntu--lv&quot;,fstype=&quot;xfs&quot;,mountpoint=&quot;/&quot;,instance=&quot;192.168.1.81:9100&quot;} &gt; 0 for: 15m labels: severity: warning AlertmanagerConfig這部分會與原本的 secret alertmanager-prometheus-operator-kube-p-alertmanager 一起使用，定義的內容都會是在 secret 內部的 (所以要改 global 的設定需要去調整 secret)要注意的是在 route 的部分，採用這個方式建立的都會自動產生一個 matcher: namespace 來讓每一條 config 都只負責自己的 namespace以下是簡易設定 slack 通知的 12345678910111213141516171819202122232425262728293031apiVersion: monitoring.coreos.com/v1alpha1kind: AlertmanagerConfigmetadata: name: config-example labels: alertmanagerConfig: examplespec: route: groupBy: ['alertname', 'job'] # 報警分組依據 groupWait: 10s # 最初即第一次等待多久時間傳送一組警報的通知 groupInterval: 10s # 在傳送新警報前的等待時間 repeatInterval: 12h # 傳送重複警報的週期 receiver: 'slack-example' # 傳送警報的接收者的名稱，以下receivers name的名稱 receivers: - name: 'slack-example' slackConfigs: - sendResolved: false apiURL: name: 'slack-config' key: 'urlSecret' channel: '#alertmanager'---# 這裡的 urlSecret 要到 slack 去設定取得apiVersion: v1kind: Secrettype: Opaquemetadata: name: slack-configdata: urlSecret: ********************** 小結這邊算是做一個簡易的紀錄，把各元件基本的設定寫下來，但實際上要先理解原本 prometheus/alertmanager 後才有辦法設定這些，這邊網路上的資源蠻多的，之後再來整理整理 參考資料Prometheus Operator — 為 Kubernetes 設定及管理 Prometheus | by smalltown | getamis使用Operator管理Prometheus - prometheus-book (gitbook.io)通过ServiceMonitor创建服务发现 (aliyun.com)","link":"/2022/09/14/Prometheus-operator-prometheus-stack-%E7%B0%A1%E4%BB%8B/"},{"title":"Linux Cgroup v2 簡易實作 CPU 管理","text":"前言在目前環境中很常使用到的 容器化 主要就是透過 Linux kernel 的資源隔離來做到的因此要弄懂容器怎麼做到資源管理，就必須要理解 Cgroup (control group) Linux namespace, overlay2 等等的那這一篇會聚焦在運算資源 (CPU) 上面做一個很簡單的實作 Cgroup v2 (control group)CGroup 所管理的對象是 process (PID) 。用來限制、隔離資源使用，分配資源其中可分配的資源包含：CPU 的時間，記憶體，網路裝置等等，可以在系統上輸入指令cat /sys/fs/cgroup/cgroup.controllers會出現 cpuset cpu io memory hugetlb pids rdma misc 等等的本篇會聚焦於 cpuset cpu 這兩項 之前第一次用的時候使用 ubuntu 20.04，預設還是使用 Cgroup v1，不過在 ubuntu 22.04 (kernel 5.15) 下已經改成預設使用 v2 了 環境描述 在 Win10 使用 VMware Workstation 16 Player 開啟虛擬機 虛擬機規格不拘，但我的環境是使用 2 CPU 來測試 虛擬機 OS: ubuntu 22.04 kernel 版本: 5.15.0-39-generic 測試檔案測試的方式是透過 yes 這隻程式會不斷產生資料的特性，再使用 timeout 來控制時間，透過比對產生的資料量來比較說是否有限制成功 process 存取 CPU 的時間測試一共會進行兩次，第一次是無限制的情況，第二次則是加上 Cgroup 來控制這邊先附上程式，說明完之後再來看執行結果 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bash# cpuset.sh function initabc(){ LOW_DIR_PATH=&quot;/sys/fs/cgroup/low&quot; [ -d &quot;$LOW_DIR_PATH&quot; ] &amp;&amp; sudo rmdir &quot;$LOW_DIR_PATH&quot; sudo mkdir &quot;$LOW_DIR_PATH&quot; CGROUPOPTION=$(cat /sys/fs/cgroup/low/cgroup.controllers) echo &quot;$LOW_DIR_PATH control: $CGROUPOPTION&quot; echo &quot;every test will use 3 sec for 'yes' command, and record result to result.txt&quot; cat /dev/null &gt; result.txt}function testabc(){ for i in $(seq 1 3) do timeout 1 yes &gt; test.txt du -h test.txt | cut -d $'\\t' -f1 &gt;&gt; result.txt done}function clearabc(){ rm test.txt echo 'test.txt removed!'}echo $1if [ &quot;$1&quot; = &quot;clear&quot; ] ;then clearabc exit 0fiinitabcecho '---origin size result---' &gt;&gt; result.txtecho 'test1...'testabcecho $$ | sudo tee &quot;$LOW_DIR_PATH/cgroup.procs&quot; &gt; /dev/nullecho 0 | sudo tee &quot;$LOW_DIR_PATH/cpuset.cpus&quot; &gt; /dev/nullecho 20000 100000 | sudo tee /sys/fs/cgroup/low/cpu.max &gt; /dev/nullecho '---after cgroup control---' &gt;&gt; result.txtecho 'test2...'testabcecho 'test finish, can check result.txt, here is result:'cat result.txt 說明一開始可以先使用 mount | grep cgroup 來檢查 Cgroup 在系統上的掛載點，以及是否使用 v2 版本 在 /sys/fs/cgroup 這個目錄下有很多檔案，而如果在該目錄新增新的目錄的話，會發現裡面自動多了很多檔案，我們就是透過這些檔案來使用 Cgroup要注意這個目錄預設是由 root 所擁有，因此要修改的話就一般使用者必須使用 sudo 這裡面的檔案會根據 cgroup.controllers, cgroup.subtree_control 這兩個檔案做變動，先對這兩個檔案做一個簡單的解釋： cgroup.controllers 裡面會記錄說目前設定要管理的資源 有 cpuset cpu io memory pids 等等， cgroup.subtree_control 設定子目錄實際上管理什麼資源 為 cgroup.controllers 的子集 echo '+pids -memory' &gt; x/y/cgroup.subtree_control 這個是 man page 裡面的範例，這個檔案會去檢查字串然後調整要加入的內容 (也就是加入用 + 扣除用 -) 我們先把我們需要管理的 cpu cpuset 加進根目錄的檔案 /sys/fs/cgroup/cgroup.subtree_control 12echo &quot;+cpu +cpuset&quot; | sudo tee /sys/fs/cgroup/cgroup.subtree_controlcat /sys/fs/cgroup/cgroup.subtree_control # 看一下有沒有加成功 接下來需要一個子目錄用來測試用，在我的例子裡是取名叫做 low ，並且對其中的三個檔案增加內容 cpuset.cpus ：指定要使用哪一顆 CPU (從0開始算) 以我的環境有雙核，該檔案預設為 0-1 ，若要指定使用第一顆核心就改成 0 cpu.max ：使用兩個參數來控制 CPU 的占比 預設為 max 100000 而我設定 20000 100000 代表最多只能使用到 20% 的時間 cgroup.procs ： 放入要控制的 PID, 在程式碼中使用 $$ 來帶入 會將該 PID 底下的所有 sub-process 自動加入 1234567sudo mkdir /sys/fs/cgroup/low# 帶入 PID, 這邊使用 $$ 是程式的寫法echo $$ | sudo tee &quot;$LOW_DIR_PATH/cgroup.procs&quot; &gt; /dev/null# 指定用第一個核心echo 0 | sudo tee &quot;$LOW_DIR_PATH/cpuset.cpus&quot; &gt; /dev/null# 占比使用 20%echo 20000 100000 | sudo tee /sys/fs/cgroup/low/cpu.max &gt; /dev/null 執行結果1./cpuset.sh 產生的資料量沒有那麼精確的是 1/5 (20%)，但是可以看出 CPU 有受到限制，並且也會隨著占比的值有所區隔而如果想要確定是不是真的 20% ，可以手動的執行一個 yes 指令，然後把他的 PID 丟進去再使用 top 觀察，就可以看到 20% 了 12345# 執行兩個，把其中一個的 PID 記下來丟進去yes &gt; /dev/null &amp;yes &gt; /dev/null &amp;echo &quot;8566&quot; | sudo tee /sys/fs/cgroup/low/cgroup.procstop # 看看是不是一個占用 100% 一個占用 20% 上下 後記這篇的內容主要算是複習用，東西很簡單很單純，不過不常使用的話還是很容易忘記做一點簡單的例子來讓自己加深印象~ 參考資料cgroups(7) - Linux manual page (man7.org)详解Cgroup V2 | Zorro’s Linux Book (zorrozou.github.io)Redhat 文件","link":"/2022/07/03/Linux-Cgroup-V2-%E7%B0%A1%E6%98%93%E5%AF%A6%E4%BD%9C/"},{"title":"SRE (DevOps) 如何定義，SLI、SLO、SLA 是什麼?","text":"前言身為一個想要應徵 SRE(Site Reliability Engineering) 工程師的我，最近被朋友問到說 SRE 的工作內容要做什麼，居然沒辦法解釋得很清楚，也代表我對於這個名詞還不夠熟悉吧…因為 SRE 也不是什麼多新的詞了，網路上不少資源可以翻閱，不過之前常常看過之後覺得自己會了，然後被問又講不清楚，所以就想說不如自己整理一篇文章吧，這樣至少忘記的時候可以快速複習一下~SRE 是由 Google 所提出的一個應用與實現，其中會使用 SLO，SLA，SLI 三項來評估服務的 可靠性。 服務水準指標 (Service-Level Indicator, SLI) 顧名思義，SLI 是用來定義服務水準的，也就是說服務的品質是好或壞就會使用這邊的定義來決定。 SLI 範例： 如 300ms 的請求延遲。或是 HTTP 狀態碼為 200 的回應次數，佔總回應次數的比率。 請求延遲，系統吞吐量，請求失敗占比等，都是可以被拿來測量 SLI 的指標。有了指標，就可以來定義系統是否有到達一定的服務水準。 服務水準目標 (Service-Level Objective, SLO)測量了 SLI 之後，就可以定義一個更明確的目標來評估服務的品質是否有達到要求。SLO 是由以下三種元素組成：**SLI、一段時間區間、目標 (通常以百分比呈現)**。 SLO 範例：在一個月之中，99.9% 的請求延遲有在 300ms 內。 也就是說是 SLI 的指標加上了時間與目標，即為 SLO。另外要注意到，實務上不太可能追求 100% 的可用，原因是成本太高並且沒有必要。過高的可靠性會拉長開發時程導致更新困難，可能就有一些新功能不好上線 (違反 SRE 快速交付的原則)。 服務水準協議 (Service-Level Agreement, SLA)SLA 與 SLO 很接近，但 SLA 更像是給客戶的承諾，其中會承諾其 SLO 應在一段時間內達到特定水準，若未達到保證的目標則會產生罰則。通常在 SLA 中定義的 SLO 會比內部的 SLO 再寬鬆一點： SLA 範例：給客戶承諾一個月內的可用性 SLO(SLA) 為 99.9%，而內部 SLO 為 99.95% (較嚴謹) SRE (DevOps) 定義在描述完三項指標之後，再來定義一下所謂 SRE 工程師 到底要負責些什麼。而很常與 SRE 綁在一起的 DevOps 網路上的定義很多，整理如下： DevOps 指的是一種文化，打破 Dev 與 Ops 溝通上的問題SRE 為 DevOps 的實踐 那麼在 DevOps 文化上，有5個基本的大方向原則： Accept failure as normal (接受失敗是常態)：定義 SLO 時不可能會有 100% 好的服務，再檢討過程中要顧及：人、文化、心理上的安全感，以及不責罵的文化。 Reduce organisational silos (降低組織隔閡)：SRE 的實踐就是在維運與 product team 共同分擔。 Implement gradual change (漸進式更動)：如金絲雀進退版，導入 CI/CD 等，降低錯誤的 cost。 Leverage tooling and automation (利用工具與自動化)：把重複做的事情自動化。 Measure everything：能夠監控服務上的指標等等，並且定義 SLA 的數值。 SRE 實作了 DevOps 的精神，希望可以降低 Dev 與 Ops 兩個部門的衝突： Dev：想要最新最酷的技術，有新東西就想要趕快上線看看。 Ops：機房能不動最穩定，連重開機都不太想，想達到 100% 的可用性。 所以這兩個部門的衝突要由 SRE 工程師來解決： 自動化：透過自動化來提高效率，並考慮導入 CI/CD 等流程建置自動化流程。 經常檢討：為提高可用性去分析及檢討根本的問題。 事前演練 ：事故發生時透過事先演練，達到預防勝於治療的成效。 SRE 的任務先看一段 Red Hat 官網對 SRE 的描述： SRE teams are responsible for how code is deployed, configured, and monitored, as well as the availability, latency, change management, emergency response, and capacity management of services in production.(如何部屬、設定、監控 Code，同時兼顧可用性，延遲，緊急回應，空間管理等) 簡單來說，SRE 的任務包含了： Availability (可用性) Latency (延遲) Performance (效能) Efficiency (效率) Change management (變更管理) Monitoring (監控) Incident response (事故應變) Capacity planning (容量規劃) 總結因為最先提出 SRE 的為 Google 的工程師，所以一開始提到的三項服務水準 (SLI, SLO, SLA) 很重要，可以作為是基本評估服務的標準。再來搞清楚 SRE 工程師實際上在公司的定位，雖然說好像會依照每家公司的定義而會有差異，但我先找到我自己的定義然後來實踐練習，也比較能說出一點東西。最後在 SRE 的觀點中，有應用了 不責罵原則，當錯誤產生時不去責罵任何一個人，而是針對錯誤進行紀錄，並且文件化，讓下次遇到類似狀況時能有個依據並且快速處理。以 Team 的方式去看待每一次失敗，失敗沒什麼，如何修正與預防才是關鍵。 以上就是一個新手小白對於 SRE 的理解，如果內容有疑慮歡迎與我討論 (批評) 。 參考資料SRE 必修課：一次搞懂 SLI、SLO、SLA 差異，Google DevOps 理念實踐 - iKala CloudSRE 是什麼？ 維運管理與 SRE 的關係 - Cloud Ace 技術部落格 (cloud-ace.tw)What is SRE? (redhat.com)(YouTube) What is DevOps? REALLY understand it | DevOps vs SRE","link":"/2022/08/12/SRE-%E5%A6%82%E4%BD%95%E5%AE%9A%E7%BE%A9%EF%BC%8CSLO%EF%BC%8CSLA%EF%BC%8CSLI-%E6%98%AF%E4%BB%80%E9%BA%BC/"},{"title":"初始教學頁面保留一下","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/09/02/hello-world/"},{"title":"nginx images 的 docker-entrypoint.sh 閱讀","text":"前言因為現在 Cloud-native 的觀念越來越盛行，container image 是基礎如果要自己寫 image 的話，就有一些眉角需要注意一下，其中一個我覺得蠻重要的就是 entrypoint 的設計 (如果設計不好可能 container 隨時就掛了) 之前也有用過 mysql 的 image, 他提供了一個 docker-entrypoint.d 來讓使用者放入 .sh,.sql 檔案，這樣在啟動的時候就可以預先載入使用者寫好的資料表，之後有機會再來深入探究那這次使用的 image 為 dockerhub.io/nginx:1.20先使用 docker inspect nginx:1.20 來觀察一下他的 entrypoint, cmd 分別是怎麼設計的 /entrypoint.sh 觀察 可看到 entrypoint 為 /docker-entrypoint.sh 這隻程式cmd 則為 nginx -g daemon off接下來就可以直接看 /docker-entrypoint.sh 是如何設計的 /docker-entrypoint.sh 完整檔案 1234567891011121314151617181920212223242526272829303132#!/bin/sh# vim:sw=4:ts=4:etset -eif [ -z &quot;${NGINX_ENTRYPOINT_QUIET_LOGS:-}&quot; ]; then exec 3&gt;&amp;1else exec 3&gt;/dev/nullfiif [ &quot;$1&quot; = &quot;nginx&quot; -o &quot;$1&quot; = &quot;nginx-debug&quot; ]; then if /usr/bin/find &quot;/docker-entrypoint.d/&quot; -mindepth 1 -maxdepth 1 -type f -print -quit 2&gt;/dev/null | read v; then echo &gt;&amp;3 &quot;$0: /docker-entrypoint.d/ is not empty, will attempt to perform configuration&quot; echo &gt;&amp;3 &quot;$0: Looking for shell scripts in /docker-entrypoint.d/&quot; find &quot;/docker-entrypoint.d/&quot; -follow -type f -print | sort -V | while read -r f; do case &quot;$f&quot; in *.sh) if [ -x &quot;$f&quot; ]; then echo &gt;&amp;3 &quot;$0: Launching $f&quot;; &quot;$f&quot; else # warn on shell scripts without exec bit echo &gt;&amp;3 &quot;$0: Ignoring $f, not executable&quot;; fi ;; *) echo &gt;&amp;3 &quot;$0: Ignoring $f&quot;;; esac done echo &gt;&amp;3 &quot;$0: Configuration complete; ready for start up&quot; else echo &gt;&amp;3 &quot;$0: No files found in /docker-entrypoint.d/, skipping configuration&quot; fifiexec &quot;$@&quot; 接下會以兩個區段來看，分別是： 基本設定區段 docker-entrypoint.d 目錄檢查區段 1.基本設定區段1234567891011121314#!/bin/sh# vim:sw=4:ts=4:et# set -e 遇到錯誤直接結束程式set -e# 檢查變數if [ -z &quot;${NGINX_ENTRYPOINT_QUIET_LOGS:-}&quot; ]; then exec 3&gt;&amp;1else exec 3&gt;/dev/nullfi# 這邊先這樣，主要是搭配最後的 exec if [ &quot;$1&quot; = &quot;nginx&quot; -o &quot;$1&quot; = &quot;nginx-debug&quot; ]; then...exec &quot;$@&quot; 先把開頭跟結尾分開來第二行的 vim:sw=4;ts=4;et 是跟 vim 編輯器縮排的有關set -e 則是當程式內有回傳非 0 時 (表示執行失敗) 就立刻結束接下來的 if [ -z &quot;${NGINX_ENTRYPOINT_QUIET_LOGS:-}&quot; ] 則是檢查這個環境變數是否有存在，有的話則會把 Log 透過管線丟出來，否則就丟到 /dev/null最後透過 $1 來檢查 cmd 是帶什麼，如果是 nginx, nginx-debug 其中兩個就會開始執行裡面主要的邏輯判斷在執行 docker run 時如果沒帶入任何參數，則預設為 nginx -g daemonoff 就符合 nginx 的情況最後放進 exec &quot;$@&quot; 代表任何帶入的 cmd 都會執行舉個例子，假設我使用 docker run -it nginx:1.20 cat /docker-entrypoint.sh 來開啟 container根據 entrypoint 的設定，還是會執行 docker-entrypoint.sh, 而這邊的 $1 就變成了 cat, if 就不會成立就會執行最後的 exec &quot;$@&quot;, 所以執行完 cat 指令後 container 就會關掉了這個應該就是增加 image 彈性的關鍵，不要讓使用者亂輸入 cmd 之後，image 整個壞掉 docker-entrypoint.d 目錄檢查區段延續上面最後的 if [ &quot;$1&quot; = &quot;nginx&quot; -o &quot;$1&quot; = &quot;nginx-debug&quot; ]; then 1234567891011121314151617181920212223242526...if [ &quot;$1&quot; = &quot;nginx&quot; -o &quot;$1&quot; = &quot;nginx-debug&quot; ]; then# 檢查是否有 '至少一個' 檔案 if /usr/bin/find &quot;/docker-entrypoint.d/&quot; -mindepth 1 -maxdepth 1 -type f -print -quit 2&gt;/dev/null | read v; then echo &gt;&amp;3 &quot;$0: /docker-entrypoint.d/ is not empty, will attempt to perform configuration&quot; echo &gt;&amp;3 &quot;$0: Looking for shell scripts in /docker-entrypoint.d/&quot; find &quot;/docker-entrypoint.d/&quot; -follow -type f -print | sort -V | while read -r f; do case &quot;$f&quot; in *.sh) if [ -x &quot;$f&quot; ]; then echo &gt;&amp;3 &quot;$0: Launching $f&quot;; &quot;$f&quot; else # warn on shell scripts without exec bit echo &gt;&amp;3 &quot;$0: Ignoring $f, not executable&quot;; fi ;; *) echo &gt;&amp;3 &quot;$0: Ignoring $f&quot;;; esac done echo &gt;&amp;3 &quot;$0: Configuration complete; ready for start up&quot; else echo &gt;&amp;3 &quot;$0: No files found in /docker-entrypoint.d/, skipping configuration&quot; fifi 當確定好 $1 = nginx 後，會再使用 find 指令來檢查 /docker-entrypoint.d/ 目錄，並且帶入 -mindepth 1 -maxdepth 1 確保只會找到 /docker-entrypoint.d/ 目錄內的檔案 (如 /docker-entrypoint.d/a/a.sh 就不會被找到)但第一個 if 的 find 還帶入了 -quit, 用意只是確保 /docker-entrypoint.d/ 目錄有 至少一個 .sh 檔案需要執行接下來確定有檔案後就執行 find &quot;/docker-entrypoint.d/&quot; -follow -type f -print | sort -V | while read -r f; 開始把所有的檔案抓出來執行裡頭使用了 case 來把 .sh 的檔案與其他的檔案區隔開來，只執行 *.sh 並且擁有執行權限的檔案 ( 使用 [ -x &quot;$f&quot; ] 檢查)while read 結束後印出 Configuration complete; ready for start up 告知使用者這個檔案的內容大致上就是這樣 實際執行結果後面有日期的那個是 nginx 開啟的訊息，可以看到 docker.entrypoint.sh 會把每一個執行的 .sh 顯示出來，真是貼心 後記在寫 dockerfile 的時候都會覺得 entrypoint 的設計很困難，為了讓自己能寫一些不只自己能使用的 image, 參考這種官方的 image 感覺是挺不錯的 (不會的話先模仿別人總沒問題了吧!!)其中在看這份 docker-entrypoint.sh 時，覺得最後一行 exec $@ 真是精隨，這樣就不會在使用者亂給 cmd 時顯示不該有的錯誤了 (當然也是搭配了上面的 if 來判斷才行)這篇文章大概就到這邊，如果有大大發現我寫的內容有問題歡迎指正我~~ 參考資料find man page stackoverflow: set -e dockerhub nginx 頁面","link":"/2022/04/18/nginx/"},{"title":"真正的 home brew 測試","text":"This template explains our QA process for shipping bug-free software. brew tap homebrew/boneyard 有一個好用的指令 versions ，但現在不支援了 看到一篇文說 ，似乎把舊版的repo 都刪掉了 I read all the answers. It’s 2020 and there is no easy way or official command to do that 沒有好的 downgrade 方法 😢 第十三章、Linux 帳號管理與 ACL 權限設定","link":"/2021/09/02/%E7%9C%9F%E6%AD%A3%E7%9A%84-home-brew-%E6%B8%AC%E8%A9%A6/"},{"title":"有顏色的shell：zsh 簡易安裝與使用心得","text":"為何要使用 zsh?先上結果 之前在看一些 it 邦幫忙的文章的時候，就有發現有不少人的 shell 色彩繽紛 而且重點是他們的 GIF 圖的範例裡面，有時候指令就莫名其妙的打完了 (不是使用 Tab) 於是呢，就想說也來練習看看，但他好像也不是什麼新東西了 單純做個筆記分享一下 2022/6/19 更新 Ubuntu Desktop 22.04 安裝基本上跟下面的步驟差不多，不過 zsh 的部分就不用再抓原始碼編譯了，可以直接採用 apt install 來安裝 1234567891011121314151617181920212223242526272829# ubuntu desktop 22.04 最小化安裝sudo apt updatesudo apt install openssh-server git# oh-my-zsh sudo apt install zsh # zsh 5.8.1sh -c &quot;$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;## p10kgit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k# 安裝 you-should-usegit clone https://github.com/MichaelAquilina/zsh-you-should-use.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/you-should-use# 在 ~/.zshrc 的 plugin 中新增 you-should-use# zsh-autosuggestionsgit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions# 在 ~/.zshrc 的 plugin 中新增 zsh-autosuggestions# zsh-completions# 有點不太確定是不是已經預設好了，我把它刪掉之後還是有 completions 的功能# git clone https://github.com/zsh-users/zsh-completions ${ZSH_CUSTOM:=~/.oh-my-zsh/custom}/plugins/zsh-completions# 下載我自己的 config , 這段可跳過# 2022.6 還是可以直接使用 ~git clone https://github.com/RayLin9981/config.gitcp config/.p10k.zsh .cp config/.zshrc .# 安裝程式碼紀錄1234567891011121314151617181920212223242526272829303132333435363738# centos 7 2021.11 測試# 使用 centos 7.4 最小版安裝# 確保 zsh 版本高於 5.1 ( centos 7 預設的 repo 的只有5.0.1的樣子)# 建議 sudo 設定 NOPASSWD 比較輕鬆安裝sudo yum update -ysudo yum install -y git make ncurses-devel gcc autoconf man yodl wget# 去下載 zsh-5.7.1 的版本來編譯git clone -b zsh-5.7.1 https://github.com/zsh-users/zsh.git /tmp/zshcd /tmp/zsh./Util/preconfig./configuresudo make -j 20 installcd ~# 安裝 ohmyzshwget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.shyes | sh install.sh# p10kgit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k#在 ~./zshrc 中加入 ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;# 接下來重新載入 zsh 會進入設定模式，可依據自己的喜好選擇，後生成 ~/.p10kzsh# 安裝 you-should-usegit clone https://github.com/MichaelAquilina/zsh-you-should-use.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/you-should-use# 在 ~/.zshrc 的 plugin 中新增 you-should-use# zsh-autosuggestionsgit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions# 在 ~/.zshrc 的 plugin 中新增 zsh-autosuggestions#zsh-completionsgit clone https://github.com/zsh-users/zsh-completions ${ZSH_CUSTOM:=~/.oh-my-zsh/custom}/plugins/zsh-completions# 下載我自己的 config , 這段可跳過git clone https://github.com/RayLin9981/config.gitcp config/.p10k.zsh .cp config/.zshrc . 可以看到有安裝了 zsh , ohmyzsh,powerlevel10k, zsh-autosuggestions , zsh-completions 等等，其實並不多，但我覺得用起來跟 bash 比起來還是方便很多 Oh My ZSH!這個開源程式主要是用來管理 zsh 的一些 plugins , 等等要安裝的 plugins 都會丟到 ~/.oh-my-zsh 的目錄裡面 Oh My ZSH 似乎是導致 zsh 開始變紅的一個因素之一，有看過一篇文是說 Oh My ZSH 出來之後在 google 搜尋上 zsh 的搜尋頻率提升很多 (離題了) 並且可以到他的官方網站去挑選主題來用，主題多到用滾輪滑也要滑很久才滑的完 但我的主題是使用 powerlevel10k powerlevel10k主要讓畫面變得鮮豔的道具~ 在第一次下載時，到 在 ~./zshrc 中加入 ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; 後進入 zsh 就會有一個互動介面，可以依循他的步驟挑選自己喜歡的模樣 結束後若想再重新設定一次 ，可輸入 p10k configure 再進入互動介面 若是想要更客製化，可以去編輯 ~/.p10k.zsh 這個檔案，裡面有非常多的東西可以修改 e.g. POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS 這個變數顧名思義就是要控制畫面右方顯示的東西，以我的畫面為例，我有新增了 ip 這個功能，就可以看到目前主要的 interface 的 IP ，省去我每次在多台電腦切來切去的時候都會忘記 IP 是多少 能客製化的東西非常多，也可以監控 CPU,memory 等系統資源，也可以自己定義圖示 (就不用顯示醜醜的 CPU 三個字)，不過我就沒有多加研究，這個可以到網路上找別人的範本來改 you-should-use下載完之後，到 ~/.zshrc 的 plugins 新增 you-should-use (用空白隔開，或是一行一行的也可以) 那這個套件呢，主要是搭配上 Linux 的 alias 功能，非常實用 因為在 ~/.zshrc 中的 plugins 有新增指令的話，他就會幫忙產生很多好用的 ailas，以我的檔案來說： 1234plugins=(git you-should-use zsh-autosuggestions zsh-completions kubectl ) # 請注意 kubectl 這行alias | grep kubectl | wc -l94 他自動幫我設定這些，就算很好用也根本記不住，但是 you should use 就會在我執行的時候告訴我應該要用有設定的 ailas ，被多提醒幾次之後要不記住也難啦~ 所以上面的指令kubectl apply -f ~/test 就可以簡化成 kaf ~/test 輕鬆又自在 zsh-autosuggestions &amp; zsh-completionzsh-completion 這個就是跟 bash-completion 差不多的功能，只要拔掉 Tab 就會立刻喪失能力的我，跳到 zsh 來也是立刻就先把這個準備好了 簡單說一下，他可以跟 kubectl (當然指令大多數都可以) 互動，比如我打 kubectl get pods -n [Tab] 的時候，他就會顯示 -n 能輸入什麼參數 (也就是我目前環境內的 namespace)，不裝他的話真的一堆指令都記不住了 XD zsh-autosuggestions 也是很好用的一個 plugins , 他會依據你曾經輸入過的指令給你建議 (用比較淺的顏色) ，如果那是你要的就直接按 方向鍵的右鍵 ，他就會自動幫你補全 預設是只有去追蹤 history ，但我覺得 completion 的功能也很好用，所以就有補上去 可以到 .oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh 裡面的 ZSH_AUTOSUGGEST_STRATEGY=(history completion ) 做這樣的調整 總結以現在一個月左右學習 kubernetes 用起來，zsh 還蠻舒服的，善用這些 plugins 的確可以減短打很多字的時間呢~ 這篇文章比我想像中的還晚生出來 XD” ，接下來會繼續加把勁的~~ 參考資料zsh安裝主要參考文章 改變windows 編碼，不然會看不到 powerlevel 的字型設定 p10k安裝字型等 VMware player 開機順序調整","link":"/2021/11/15/%E6%9C%89%E9%A1%8F%E8%89%B2%E7%9A%84shell%EF%BC%9Azsh-%E7%B0%A1%E6%98%93%E5%AE%89%E8%A3%9D%E8%88%87%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97/"},{"title":"第一篇：自我介紹與部落格方針","text":"前言大家好，我是林禹志，這是這網站的第一篇文，會簡單的描述一下我的心路歷程與創建部落格的規劃 首先我必須要說，因為我還很菜沒有什麼實務經驗，所以以下的內容會依照我目前的理解來寫，可能會有很多錯誤的部分，敬請見諒~ 第一次使用雲端 (AWS)第一次接觸到雲端是大學時系上有一位教授 (黃懷陞 教授，下稱 Cage ) 是 AWS 的合作夥伴，利用 AWS educate 的資格開了兩門選修課，我對雲端很感興趣就選了 在那時還不懂網路架構等等的東西，所以用到最多的就是 EC2 當作虛擬機使用而已，其他服務就是聽聽理論 (如： AWS S3 能夠給予幾個9的可用性等等的)，有點沒辦法融會貫通。不過我認為雲端是未來的趨勢，後來也請 Cage 作為我們專題的指導教授，繼續跟著他學習。 第一次參加社群活動 (2019 COSCUP)能夠接觸到這塊，也是 Cage 要我們去找找專題的靈感。那時候我還沒有接觸過 Linux ，甚至連 Ubuntu 也沒聽過，有點像劉姥姥進大觀園，幾乎什麼都看不懂聽不懂 XD 其中有一場議程讓我比較印象深刻，主題叫 ：「帶你了解 grep 的背後原理」，想當然，我那時也沒用過這個指令，我就給了自己一個小小的目標 「我明年的 COSCUP，至少要知道 grep 是什麼!」 現在來看，確實是一個非常小的目標，但那時候我還很習慣老師教什麼我就學什麼，還不太會自己上網學習新的東西(還待在舒適圈) 總之，開啟了一個很大的門，裡面有著無數的知識等著挖掘。 實習安排的社群大大演講收穫在一年的實習中，總算是學習到了比較完整的 Linux 操作 (Red Hat) ，不過實習的內容想要下一段再講，這邊想先講社群大大的演講。 公司邀請到了 Phil 與 Rico 兩位講師，現在發現都是在 CNTUG 的大大們呢~ Rico 主要是講如何貢獻開源專案(openstack)，但因為我現在還沒有開始貢獻，所以還沒有什麼體悟 倒是 Phil 主要是談論自學的一些經驗等等，現在比較有感 Phil 在剛開始講的時候就說 ： 「我要講的東西很困難，要像我一樣有病才有可能堅持下去」 「我不期望我講完能讓台下所有人都動起來，現在台下大概20幾個人，我只要能啟發一個人，這場演講就成功了」 我大概也不算被啟發的，畢竟也是過了快一年多才發覺到自學的重要性 「喔還有，我通常只記得女生，男生的話不要過來跟我裝熟」 演講內容其實大概是分享一下 Phil 讀研究所的一些經歷等等，還有可以去改 .vimrc 當作自學的目標啊之類的 我記最深的是，QA時我問 Phil ：「我最近在自學 D Jango (發音錯)，不知道您能不能給我一些建議？」 他就回我：「你發音錯了吧？ django?」 然後也沒回答我的問題，大概說了三次我發音錯了XD 還有我回答問題，Phil 有準備貼紙獎勵，過去一看都是沒看過的東東，就選了一個可愛的三地鼠(podman) Phil 就跟我分享一下最近他有在研究這個 pod 管理程式這樣 當時聽的似懂非懂，不過現在才知道原來 podman 基本用起來跟 docker 差不多呢 (基本) 大概是這樣 實習時，自學openstack的痛苦經歷沒錯，是痛苦的經歷，有點像是越級打怪這樣，一大堆看都看不懂的名詞，安裝文件裡面預期有的基礎知識我幾乎都沒有，所以就只能看一看，然後照著文件內的指令照做，就變成了以下的循環： 「欸，失敗了欸，怎麼會失敗呢」 (把錯誤訊息丟 google ，隨便亂改參數或.yaml檔再執行) 「欸！成功了欸！怎麼會成功呢！？」 這樣的流程大概跑了10-20次吧，搞了半年，總算是把 Red Hat openstack 的基本架構架起來了 然後，實習就結束了，現在回去看我那時候的安裝筆記，裡面就全部是我改了哪些檔案，哪個參數等等，部署的指令可能排列組合起來就有幾百種 更要命的是 ， overcloud 在部署的時候，指令跑下去有時候就要半小時 (如果參數錯在後面的步驟) 也就是我測試上百個的參數，每天就是在執行指令，然後看著黑畫面開始拜託他成功。 現在看來是一個很糟糕的自學心態，不過那時候有每個禮拜的進度壓力，再加上接學校助教每個禮拜都要備課等等的 最大的收穫大概是知道自己是有多廢吧(? ，也確立了後面激起自學的心 把那時候不懂的名詞記一下：L2,L3 ,HAproxy,Puppet,vxlan,gateway(對，就是那個gateway) , 認識 CNTUG (COSCUP 2021)今年因為疫情的關係，COSCUP 改為線上舉辦，改使用 Gather town 進行議程，每一個議程軌也改成講師預先錄好影片後再播放 我就和前兩年參與的模式一樣，先規劃一下有興趣的主題，再來看看要看什麼，注意到了大部分我有興趣的主題都是 CNTUG 的議程軌 其中 Gene Guo 大大的 「從零打造自己的 Home Lab」 主題很吸引我，因為正有打算來自建一個 Home Lab 來練習 openstack 用 (他的 lab 也是有用 openstack 架構) 不過內容還是有很多聽不懂的部分，我也盡可能的做成筆記囉 最重要的是，受惠於線上舉辦，我也總算鼓起勇氣在虛擬會場上找到了 Gene Guo 大大聊聊。 主要就提到了前面 openstack 的慘痛學習經驗，他也給了我一些關鍵字(OVS,OVN)等，也建議我可以一個一個元件來裝 (RHEL 的就是一次全包) ，再來慢慢了解。 日後的學習方針與部落格規劃上面就大概講了一下會走到現在這步(? 的情況 在前面這段時間(2021/7-9) ，把 Cisco CCNA 證照的內容讀了一下，總算是對網路架構有了比較完整的了解，也把 Red Hat 的 RHCSA 證照拿到手了，接下來會繼續往 docker,k8s,openstack(ovs,ovn) 等等學習，也因為我還很多東西不會，所以希望能提供給跟我一樣的人一些啟發，一起學習進步這樣 目前大概會參考 phil 的 home lab ，也來搞一個自己的文章或是先寫政府課的心得(SRE 課程，要上到11/30)，內容有包含 k8s 與 container 等，或許有什麼新的體悟也可以來更新文章 大概就是這樣，也期許自己能夠繼續堅持下去囉 感謝你對我有興趣把這篇落落長的第一篇文章看完~~~ 後記最近和一位在企業管伺服器的講師聊到 openstack ，結果他一聽到回我三個字「爛東西」 實在是一個很令人意外的答案XD” ，不過我應該還是會用用看才能知道「爛」在哪邊吧","link":"/2021/09/19/%E7%AC%AC%E4%B8%80%E7%AF%87%EF%BC%9A%E8%87%AA%E6%88%91%E4%BB%8B%E7%B4%B9%E8%88%87%E9%83%A8%E8%90%BD%E6%A0%BC%E6%96%B9%E9%87%9D/"}],"tags":[{"name":"ckad","slug":"ckad","link":"/tags/ckad/"},{"name":"cka","slug":"cka","link":"/tags/cka/"},{"name":"monitoring","slug":"monitoring","link":"/tags/monitoring/"},{"name":"prometheus","slug":"prometheus","link":"/tags/prometheus/"},{"name":"alertmanager","slug":"alertmanager","link":"/tags/alertmanager/"},{"name":"Liunx","slug":"Liunx","link":"/tags/Liunx/"},{"name":"Linux Troubleshooting","slug":"Linux-Troubleshooting","link":"/tags/Linux-Troubleshooting/"},{"name":"SRE","slug":"SRE","link":"/tags/SRE/"},{"name":"DevOps","slug":"DevOps","link":"/tags/DevOps/"},{"name":"container","slug":"container","link":"/tags/container/"},{"name":"docker-entrypoint","slug":"docker-entrypoint","link":"/tags/docker-entrypoint/"}],"categories":[]}